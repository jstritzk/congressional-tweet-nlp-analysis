{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and Merging Senator Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter API credentials\n",
    "\n",
    "consumer_key = \"BB41pRXH69PbiCnVYUAS3LlE2\"\n",
    "consumer_secret = \"ZIachunFzUG3SBV559Tyvn9UceUq3Kx0GJvtuBjp5r2wVfxSoV\"\n",
    "access_key = \"63756884-obyOilYP0zDh1PLiujaK3AFSHA57BVK2nbsEv3upY\"\n",
    "access_secret = \"YofB5X5F7KDj1qLvjp8LcicxMOZK4XPPeOf40Xqz4tlYN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the last 1000 tweets of a given twitter account\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "    alltweets = []\n",
    "    noRT = []\n",
    "\n",
    "    #make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name, tweet_mode = \"extended\", count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until the api limit is reached\n",
    "    while len(alltweets) <= 1000:\n",
    "        print(\"getting tweets before {}\".format(oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,tweet_mode = \"extended\", count=200,max_id=oldest)\n",
    "        \n",
    "        if len(new_tweets) == 0:\n",
    "            break\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...{} tweets downloaded so far\".format(len(alltweets)))\n",
    "\n",
    "    #removes retweets\n",
    "    for tweet in alltweets:\n",
    "        if \"RT\" in tweet.full_text:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            noRT.append([tweet.id_str, tweet.created_at, tweet.full_text, screen_name])\n",
    "\n",
    "    #write to csv\n",
    "    with open(\"{}_tweets.csv\".format(screen_name), \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"screen_name\"])\n",
    "        writer.writerows(noRT)\n",
    "        print(\"{}_tweets.csv was successfully created.\".format(screen_name))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all twitter names from us senators\n",
    "# dataset obtained from: https://github.com/CivilServiceUSA/us-senate/blob/master/us-senate/data/us-senate.csv\n",
    "\n",
    "senators_df = pd.read_csv(\"us-senate.csv\", sep = \";\")\n",
    "senator_twitter_handles = senators_df[\"twitter_handle\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1453526561144164362\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1388150472461127679\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1326590430276808703\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1264280012418682880\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1240437907376295941\n",
      "...1200 tweets downloaded so far\n",
      "SenDanSullivan_tweets.csv was successfully created.\n",
      "getting tweets before 1493768593493741574\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1473089828669382655\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1453145165493792769\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1432476997570211847\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1410088861695320065\n",
      "...1200 tweets downloaded so far\n",
      "lisamurkowski_tweets.csv was successfully created.\n",
      "getting tweets before 1491164058531557375\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1460697614190587908\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1440437858448265221\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1422318352165490704\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1395820550992302080\n",
      "...1200 tweets downloaded so far\n",
      "SenTuberville_tweets.csv was successfully created.\n",
      "getting tweets before 1415779217317502977\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1332689273309573119\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1245065612851888129\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1184588785151041535\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1116080149617836031\n",
      "...1197 tweets downloaded so far\n",
      "SenShelby_tweets.csv was successfully created.\n",
      "getting tweets before 1493251268143370239\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1473736609619193869\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1456287134369959941\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1438887051320045569\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1420779797639319552\n",
      "...1199 tweets downloaded so far\n",
      "JohnBoozman_tweets.csv was successfully created.\n",
      "getting tweets before 1471171656936828935\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1440295399651115007\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1408143103438032895\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1384942265597050882\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1369452534344073215\n",
      "...1200 tweets downloaded so far\n",
      "sentomcotton_tweets.csv was successfully created.\n",
      "getting tweets before 1467931853810245635\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1430555565269291010\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1393295050789081091\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1365013702563557375\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1335600118708510720\n",
      "...1199 tweets downloaded so far\n",
      "kyrstensinema_tweets.csv was successfully created.\n",
      "getting tweets before 1484603993150132224\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1460434049705689091\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1435284203969597439\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1395082769647161347\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1342172514647064575\n",
      "...1033 tweets downloaded so far\n",
      "SenMarkKelly_tweets.csv was successfully created.\n",
      "getting tweets before 1486469039409057795\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1461770686377177089\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1443999178762371072\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1426277524036796424\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1413232316319686664\n",
      "...1200 tweets downloaded so far\n",
      "SenFeinstein_tweets.csv was successfully created.\n",
      "getting tweets before 1490097909076873216\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1466561317779787782\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1446153335896150015\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1429879268188827648\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1413550206277361672\n",
      "...1200 tweets downloaded so far\n",
      "SenAlexPadilla_tweets.csv was successfully created.\n",
      "getting tweets before 1484288481803345919\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1458571530653294592\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1418744954613116928\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1389751090795753476\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1367887724531843078\n",
      "...1136 tweets downloaded so far\n",
      "SenatorHick_tweets.csv was successfully created.\n",
      "getting tweets before 1486820928302559231\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1461077871020101652\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1435715273013833729\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1415707946546339843\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1395161953413308416\n",
      "...1200 tweets downloaded so far\n",
      "SenatorBennet_tweets.csv was successfully created.\n",
      "getting tweets before 1497622038747492351\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1486708911772160001\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1476291590868058129\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1464421694882430979\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1453886615206641664\n",
      "...1198 tweets downloaded so far\n",
      "ChrisMurphyCT_tweets.csv was successfully created.\n",
      "getting tweets before 1493720085797683200\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1475262555396444165\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1453088908305223694\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1436417731495178249\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1420121835946053642\n",
      "...1200 tweets downloaded so far\n",
      "SenBlumenthal_tweets.csv was successfully created.\n",
      "getting tweets before 1480664576253472771\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1443027358341083137\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1408783703652847619\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1385405554789687296\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1366861637815304194\n",
      "...1200 tweets downloaded so far\n",
      "ChrisCoons_tweets.csv was successfully created.\n",
      "getting tweets before 1477269879426220031\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1438615735375433734\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1404543946899038208\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1385241189163716610\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1368180512662110214\n",
      "...1200 tweets downloaded so far\n",
      "SenatorCarper_tweets.csv was successfully created.\n",
      "getting tweets before 1499438114925957123\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1489277474391404546\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1478845236570992643\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1466805976351358976\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1455282119731814405\n",
      "...1200 tweets downloaded so far\n",
      "SenRickScott_tweets.csv was successfully created.\n",
      "getting tweets before 1499543192047427585\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1494089557314347011\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1466413068137680895\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1428420829981855746\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1414593311734697992\n",
      "...1200 tweets downloaded so far\n",
      "marcorubio_tweets.csv was successfully created.\n",
      "getting tweets before 1482087545261408256\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1443261183381880841\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1415768955554353159\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1392154031309660166\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1376729293154365448\n",
      "...1096 tweets downloaded so far\n",
      "SenOssoff_tweets.csv was successfully created.\n",
      "getting tweets before 1490769542796255237\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1470859766977245188\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1455254453293043717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...800 tweets downloaded so far\n",
      "getting tweets before 1440412637443944451\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1422689913938530303\n",
      "...1200 tweets downloaded so far\n",
      "SenatorWarnock_tweets.csv was successfully created.\n",
      "getting tweets before 1493987624335417346\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1476642511514918916\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1457057519218696192\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1443400993199230975\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1422744755931435008\n",
      "...1200 tweets downloaded so far\n",
      "brianschatz_tweets.csv was successfully created.\n",
      "getting tweets before 1479165205968035851\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1436439258324156417\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1390084602774052865\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1369478227836760069\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1334580093730115584\n",
      "...1200 tweets downloaded so far\n",
      "maziehirono_tweets.csv was successfully created.\n",
      "getting tweets before 1490693296267669506\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1467958780709281791\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1449180406385893379\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1426698656846057471\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1405237786324652039\n",
      "...1200 tweets downloaded so far\n",
      "ChuckGrassley_tweets.csv was successfully created.\n",
      "getting tweets before 1441092729711173633\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1363227928822243329\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1317590605804032001\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1305883368639336450\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1286344723414814719\n",
      "...1200 tweets downloaded so far\n",
      "joniernst_tweets.csv was successfully created.\n",
      "getting tweets before 1485669705239179264\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1450886226668662784\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1423007904236548096\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1395377729562873858\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1370066303424331779\n",
      "...1199 tweets downloaded so far\n",
      "SenatorRisch_tweets.csv was successfully created.\n",
      "getting tweets before 1481029801129431048\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1453843219003883521\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1425571461750788099\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1399831269467377669\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1379498809152114687\n",
      "...1200 tweets downloaded so far\n",
      "MikeCrapo_tweets.csv was successfully created.\n",
      "getting tweets before 1499135810595131392\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1486735597851099135\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1470565762112921599\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1455993861696786446\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1443686667982938111\n",
      "...1200 tweets downloaded so far\n",
      "SenatorDurbin_tweets.csv was successfully created.\n",
      "getting tweets before 1486831976455839752\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1462092769070129156\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1441421474355625985\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1417209581894242303\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1390408573021216773\n",
      "...1200 tweets downloaded so far\n",
      "SenDuckworth_tweets.csv was successfully created.\n",
      "getting tweets before 1484558677919027201\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1468193953988100112\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1455604587650588680\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1442206015622197253\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1423384223386374144\n",
      "...1199 tweets downloaded so far\n",
      "SenatorBraun_tweets.csv was successfully created.\n",
      "getting tweets before 1473756360177299456\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1430170161231839233\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1385593330734616579\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1346524520434302986\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1310945694279241730\n",
      "...1200 tweets downloaded so far\n",
      "ToddYoungIN_tweets.csv was successfully created.\n",
      "getting tweets before 1468566862959099909\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1423984955974201343\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1374398030678433797\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1328721878589038596\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1279202525665808383\n",
      "...1200 tweets downloaded so far\n",
      "JerryMoran_tweets.csv was successfully created.\n",
      "getting tweets before 1499446493572485124\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1489242174554185727\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1477761848896278527\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1466417961200193545\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1456627591868264447\n",
      "...1199 tweets downloaded so far\n",
      "RogerMarshallMD_tweets.csv was successfully created.\n",
      "getting tweets before 1491087615617290241\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1471143142439239688\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1449037805552865289\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1428414972015616002\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1411001332941602822\n",
      "...1200 tweets downloaded so far\n",
      "McConnellPress_tweets.csv was successfully created.\n",
      "getting tweets before 1491756528189284352\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1476950631646650371\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1458056622450827266\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1435403920315228168\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1413669776111587334\n",
      "...1198 tweets downloaded so far\n",
      "RandPaul_tweets.csv was successfully created.\n",
      "getting tweets before 1498092882678493184\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1487072885919342596\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1472565215153860619\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1460690126296821760\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1448680582678990856\n",
      "...1200 tweets downloaded so far\n",
      "SenBillCassidy_tweets.csv was successfully created.\n",
      "getting tweets before 805113477429915648\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 514467212229615615\n",
      "...593 tweets downloaded so far\n",
      "getting tweets before 92971866188300287\n",
      "...621 tweets downloaded so far\n",
      "getting tweets before 2514575825\n",
      "JohnKennedyLA_tweets.csv was successfully created.\n",
      "getting tweets before 1493378271462887424\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1472016006956691462\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1455955153287319553\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1440007441098448896\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1422981566599503877\n",
      "...1200 tweets downloaded so far\n",
      "SenMarkey_tweets.csv was successfully created.\n",
      "getting tweets before 1491412059598520320\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1473345017825697793\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1455248742504882176\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1438533051382894600\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1417917865760595969\n",
      "...1200 tweets downloaded so far\n",
      "SenWarren_tweets.csv was successfully created.\n",
      "getting tweets before 1497272343017803785\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1486415288757366783\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1469391214889902079\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1455582727231533055\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1441110304981585923\n",
      "...1200 tweets downloaded so far\n",
      "SenatorCardin_tweets.csv was successfully created.\n",
      "getting tweets before 1482783416227618820\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1436407061353480199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...600 tweets downloaded so far\n",
      "getting tweets before 1402422692817432575\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1367965150679072768\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1341438478874230787\n",
      "...1200 tweets downloaded so far\n",
      "ChrisVanHollen_tweets.csv was successfully created.\n",
      "getting tweets before 1459877823603167235\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1417883624272273418\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1374730747928526851\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1331297450146099203\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1285998386940915712\n",
      "...1200 tweets downloaded so far\n",
      "SenAngusKing_tweets.csv was successfully created.\n",
      "getting tweets before 1384505990222127103\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1319674506743549952\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1290382635567636482\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1258446860706025472\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1237777168870055936\n",
      "...1200 tweets downloaded so far\n",
      "senatorcollins_tweets.csv was successfully created.\n",
      "getting tweets before 1492283558274211844\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1470489640822050820\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1446864966078828543\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1421876381114421253\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1403726916197175307\n",
      "...1200 tweets downloaded so far\n",
      "SenStabenow_tweets.csv was successfully created.\n",
      "getting tweets before 1486340577406898181\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1459166532060209153\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1429948417170722820\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1405233123374342149\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1382340669058977793\n",
      "...1200 tweets downloaded so far\n",
      "SenGaryPeters_tweets.csv was successfully created.\n",
      "getting tweets before 1471856691125882880\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1433090493383065604\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1393574158399848460\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1367188478153351171\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1341508631649546247\n",
      "...1200 tweets downloaded so far\n",
      "SenTinaSmith_tweets.csv was successfully created.\n",
      "getting tweets before 1492609791478407178\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1476330641989984258\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1460433800295497738\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1448004672871145482\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1430906234429874186\n",
      "...1200 tweets downloaded so far\n",
      "amyklobuchar_tweets.csv was successfully created.\n",
      "getting tweets before 1486531172133785600\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1455177144775086084\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1424447125346299904\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1390321199981400064\n",
      "...997 tweets downloaded so far\n",
      "getting tweets before 1344432745150296075\n",
      "...1193 tweets downloaded so far\n",
      "HawleyMO_tweets.csv was successfully created.\n",
      "getting tweets before 1481030267015938053\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1456682668129718276\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1428788162214744068\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1407803593198079999\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1392240766773305344\n",
      "...1200 tweets downloaded so far\n",
      "RoyBlunt_tweets.csv was successfully created.\n",
      "getting tweets before 1496863517089144834\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1483819033455575040\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1465871871631675391\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1438922500793110543\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1419738289238515711\n",
      "...1200 tweets downloaded so far\n",
      "SenatorWicker_tweets.csv was successfully created.\n",
      "getting tweets before 1469293331322740735\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1436096246045872130\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1402736233088958469\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1370768422498361349\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1333440028916277252\n",
      "...1200 tweets downloaded so far\n",
      "SenHydeSmith_tweets.csv was successfully created.\n",
      "getting tweets before 1491502526851690496\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1470058490467164161\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1450589986307387394\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1413204785575120902\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1375927618491539458\n",
      "...1200 tweets downloaded so far\n",
      "SenatorTester_tweets.csv was successfully created.\n",
      "getting tweets before 1486436206577471488\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1465089798541094917\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1445033507806453761\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1416123902527516674\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1385699024892833795\n",
      "...1200 tweets downloaded so far\n",
      "SteveDaines_tweets.csv was successfully created.\n",
      "getting tweets before 1141710868922679295\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1039872305327992831\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 953665726530768896\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 897196023180918784\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 831548588824133631\n",
      "...1199 tweets downloaded so far\n",
      "SenatorBurr_tweets.csv was successfully created.\n",
      "getting tweets before 1474413884933029893\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1448328880255799296\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1405521791938932738\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1372641622479998983\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1329785570873270276\n",
      "...1200 tweets downloaded so far\n",
      "SenThomTillis_tweets.csv was successfully created.\n",
      "getting tweets before 1488914737022779391\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1466855428156006408\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1441162460317245443\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1423665517441736711\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1404446025474101252\n",
      "...1200 tweets downloaded so far\n",
      "SenKevinCramer_tweets.csv was successfully created.\n",
      "getting tweets before 1448743281647628294\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1389611747011616767\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1324077737179492351\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1275803164395044863\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1237469230188187649\n",
      "...1200 tweets downloaded so far\n",
      "SenJohnHoeven_tweets.csv was successfully created.\n",
      "getting tweets before 826608576378630145\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 708332503141117951\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 642400772391596031\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 593510512626794495\n",
      "...932 tweets downloaded so far\n",
      "getting tweets before 552845357068320767\n",
      "SenSasse_tweets.csv was successfully created.\n",
      "getting tweets before 1481020389283274751\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1450214196604510210\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1415021084240400393\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1376897435906215937\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1339659283668094976\n",
      "...1200 tweets downloaded so far\n",
      "SenatorFischer_tweets.csv was successfully created.\n",
      "getting tweets before 1496213458429386759\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1484270006141652991\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1471268839493038079\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1461078691685961733\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1450865526197063683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...1200 tweets downloaded so far\n",
      "SenatorShaheen_tweets.csv was successfully created.\n",
      "getting tweets before 1466803352977776639\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1425124907411779587\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1321837868076118015\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 794962387774435327\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 791068827970920447\n",
      "...1200 tweets downloaded so far\n",
      "Maggie_Hassan_tweets.csv was successfully created.\n",
      "getting tweets before 1426200706042716168\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1374379353455218699\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1336410776115048447\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1320557958418341892\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1312877866745581568\n",
      "...1200 tweets downloaded so far\n",
      "CoryBooker_tweets.csv was successfully created.\n",
      "getting tweets before 1498770382664384516\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1487838835966988289\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1476029825122975744\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1459217877484122120\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1447953046407942144\n",
      "...1200 tweets downloaded so far\n",
      "SenatorMenendez_tweets.csv was successfully created.\n",
      "getting tweets before 1477772815134826497\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1445472587002179595\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1423247780106096649\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1407055357587955713\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1392868296752603140\n",
      "...1200 tweets downloaded so far\n",
      "MartinHeinrich_tweets.csv was successfully created.\n",
      "getting tweets before 1471561963004116998\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1441496741564530688\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1417592601293688833\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1382063643420942343\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1351917463802413060\n",
      "...1200 tweets downloaded so far\n",
      "SenatorLujan_tweets.csv was successfully created.\n",
      "getting tweets before 1486434070687096840\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1460318680395522054\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1438553147727810559\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1420121565719638016\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1402713546572681215\n",
      "...1200 tweets downloaded so far\n",
      "SenCortezMasto_tweets.csv was successfully created.\n",
      "getting tweets before 1493977182481391616\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1479131799578103809\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1458213059005239302\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1443372521328451584\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1428801961806254081\n",
      "...1200 tweets downloaded so far\n",
      "SenJackyRosen_tweets.csv was successfully created.\n",
      "getting tweets before 1498423540005154828\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1488232668198223875\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1475935746842316803\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1463222113087766538\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1454595541728387073\n",
      "...1200 tweets downloaded so far\n",
      "SenSchumer_tweets.csv was successfully created.\n",
      "getting tweets before 1450233810977759237\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1415431416821714945\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1383137492543606783\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1354552048323350527\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1318648105345179648\n",
      "...1200 tweets downloaded so far\n",
      "SenGillibrand_tweets.csv was successfully created.\n",
      "getting tweets before 796110079216926719\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 793076073554644992\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 790213702305796095\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 788160262645903359\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 786295387048988672\n",
      "...1200 tweets downloaded so far\n",
      "robportman_tweets.csv was successfully created.\n",
      "getting tweets before 1473726182571102218\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1436696315078094852\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1400924581502853124\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1372566467661733887\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1354490380780244991\n",
      "...1200 tweets downloaded so far\n",
      "SenSherrodBrown_tweets.csv was successfully created.\n",
      "getting tweets before 1484229593750028287\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1460676544586649600\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1442578574393761799\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1422276140727681030\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1397341717158776834\n",
      "...1200 tweets downloaded so far\n",
      "jiminhofe_tweets.csv was successfully created.\n",
      "getting tweets before 1498308306795995142\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1484194011707588611\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1465833024298401798\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1443923662311071748\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1420854012748959745\n",
      "...1200 tweets downloaded so far\n",
      "SenatorLankford_tweets.csv was successfully created.\n",
      "getting tweets before 1498701769945268223\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1487195469377904644\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1475155633737510915\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1463237102963933183\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1450882246769004553\n",
      "...1200 tweets downloaded so far\n",
      "SenJeffMerkley_tweets.csv was successfully created.\n",
      "getting tweets before 1485697863854133247\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1448733737856888839\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1421137160720113663\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1400909368413999109\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1382459001837744127\n",
      "...1199 tweets downloaded so far\n",
      "RonWyden_tweets.csv was successfully created.\n",
      "getting tweets before 1469135436396310532\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1407438134376374274\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1329927447656194054\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1237027466465722367\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1152282285179322367\n",
      "...1197 tweets downloaded so far\n",
      "SenToomey_tweets.csv was successfully created.\n",
      "getting tweets before 1494713822107639811\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1482041129596067841\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1462835200543662087\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1451225047067209727\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1440020283289636868\n",
      "...1200 tweets downloaded so far\n",
      "SenBobCasey_tweets.csv was successfully created.\n",
      "getting tweets before 1438206931399847936\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1362417709158391809\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1320508057659543552\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1277373942064467970\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1249384707458043907\n",
      "...1198 tweets downloaded so far\n",
      "SenJackReed_tweets.csv was successfully created.\n",
      "getting tweets before 1501650176708157440\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1493225766770941952\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1483100906933653508\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1469468136495849471\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1458878111525789699\n",
      "...1200 tweets downloaded so far\n",
      "SenWhitehouse_tweets.csv was successfully created.\n",
      "getting tweets before 1496633079183646721\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1472385444570488833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...600 tweets downloaded so far\n",
      "getting tweets before 1453800154528681992\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1429246061986689026\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1411006446171348992\n",
      "...1200 tweets downloaded so far\n",
      "LindseyGrahamSC_tweets.csv was successfully created.\n",
      "getting tweets before 1487154181349810177\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1463162932397756415\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1442182441171566592\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1410995168057204739\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1383056596331335679\n",
      "...1200 tweets downloaded so far\n",
      "SenatorTimScott_tweets.csv was successfully created.\n",
      "getting tweets before 1479094814029328387\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1441096136186490893\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1410391219268632578\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1377355241121845257\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1336381854195322886\n",
      "...1200 tweets downloaded so far\n",
      "SenJohnThune_tweets.csv was successfully created.\n",
      "getting tweets before 1408792414123405317\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1329471902755282951\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1284484826184191999\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1256282952159420421\n",
      "...997 tweets downloaded so far\n",
      "getting tweets before 1237132425186217986\n",
      "...1197 tweets downloaded so far\n",
      "SenatorRounds_tweets.csv was successfully created.\n",
      "getting tweets before 1501700479058944002\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1496119484964290564\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1489404239629725698\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1483481286572486661\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1471996361960742915\n",
      "...1200 tweets downloaded so far\n",
      "MarshaBlackburn_tweets.csv was successfully created.\n",
      "getting tweets before 1500907886083411967\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1494381129801084932\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1486533167649488898\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1478781731662671881\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1468726711042854921\n",
      "...1200 tweets downloaded so far\n",
      "SenatorHagerty_tweets.csv was successfully created.\n",
      "getting tweets before 1503380770198630400\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1499086203521482751\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1494738524234526727\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1489598958112591872\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1484229191507886082\n",
      "...1199 tweets downloaded so far\n",
      "JohnCornyn_tweets.csv was successfully created.\n",
      "getting tweets before 1499813322111258631\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1492209144572563463\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1484345415268851716\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1470941426347347972\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1460631538937249792\n",
      "...1200 tweets downloaded so far\n",
      "SenTedCruz_tweets.csv was successfully created.\n",
      "getting tweets before 1467946202490671103\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1435350067075031039\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1394777380082917377\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1360000373059166210\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1315715098678374399\n",
      "...1200 tweets downloaded so far\n",
      "SenMikeLee_tweets.csv was successfully created.\n",
      "getting tweets before 1468767880359649284\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1433828633915891719\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1395764945174908927\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1345825107567112191\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1276188460945203199\n",
      "...1200 tweets downloaded so far\n",
      "SenatorRomney_tweets.csv was successfully created.\n",
      "getting tweets before 1496545894333779967\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1480714762799484931\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1461091781752377348\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1421152585550020610\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1396949131071950847\n",
      "...1200 tweets downloaded so far\n",
      "MarkWarner_tweets.csv was successfully created.\n",
      "getting tweets before 1470519669408808964\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1420003146580037638\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1375150535444213767\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1336306239849050113\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1293927725263200256\n",
      "...1200 tweets downloaded so far\n",
      "timkaine_tweets.csv was successfully created.\n",
      "getting tweets before 1476019361680969727\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1449117114980315138\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1423676422460252164\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1390714622446120968\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1364961705734918146\n",
      "...1200 tweets downloaded so far\n",
      "SenSanders_tweets.csv was successfully created.\n",
      "getting tweets before 1485694922082635782\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1451924277452840965\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1424764005822967807\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1394656629711425535\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1372888441420124165\n",
      "...1200 tweets downloaded so far\n",
      "SenatorLeahy_tweets.csv was successfully created.\n",
      "getting tweets before 1461425356104163339\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1415669661501333524\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1359970904621666310\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1310699283151294471\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1271192684678414336\n",
      "...1200 tweets downloaded so far\n",
      "SenatorCantwell_tweets.csv was successfully created.\n",
      "getting tweets before 1497349733907578882\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1484913998290059264\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1471189617139404808\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1458173297091391495\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1445869222496714754\n",
      "...1200 tweets downloaded so far\n",
      "PattyMurray_tweets.csv was successfully created.\n",
      "getting tweets before 1472035494120804356\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1442656261544230913\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1419832245397991437\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1400601304595152904\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1381814659611820031\n",
      "...1199 tweets downloaded so far\n",
      "SenRonJohnson_tweets.csv was successfully created.\n",
      "getting tweets before 1324178471325753344\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1253003350515298305\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1058422531340808193\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1051270573496692735\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1039964773000138753\n",
      "...1200 tweets downloaded so far\n",
      "tammybaldwin_tweets.csv was successfully created.\n",
      "getting tweets before 1482353375798833151\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1427621995945074691\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1351962970855776255\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1331356971442843647\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1313229131585331199\n",
      "...1200 tweets downloaded so far\n",
      "Sen_JoeManchin_tweets.csv was successfully created.\n",
      "getting tweets before 1491172524075679743\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1468695199702470664\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1450129084244283394\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1427033281099444242\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1407692238512168960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...1200 tweets downloaded so far\n",
      "SenCapito_tweets.csv was successfully created.\n",
      "getting tweets before 1483921732545175552\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1455697606517067775\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1435966896805875720\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1414678578348843015\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1392529012484890626\n",
      "...1200 tweets downloaded so far\n",
      "SenJohnBarrasso_tweets.csv was successfully created.\n",
      "getting tweets before 1486718890453323778\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1454920836096438272\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1424556794295189504\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1400625929244663812\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1379439742396346367\n",
      "...1200 tweets downloaded so far\n",
      "SenLummis_tweets.csv was successfully created.\n"
     ]
    }
   ],
   "source": [
    "# getting last 1000 tweets for all senators from list\n",
    "\n",
    "for twitter_handle in senator_twitter_handles: \n",
    "    try:\n",
    "        get_all_tweets(twitter_handle)\n",
    "    except: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge individual tweet files\n",
    "\n",
    "individual_tweets_filenames = [i for i in glob.glob(\"*tweets.csv\")]\n",
    "combined_tweets = pd.concat([pd.read_csv(f) for f in individual_tweets_filenames ])\n",
    "combined_tweets.to_csv( \"tweets_combined.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete individual tweet files\n",
    "\n",
    "for file in individual_tweets_filenames: \n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Senator Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and renaming merged tweet file \n",
    "\n",
    "df = pd.read_csv(\"tweets_combined.csv\")\n",
    "df = df.rename(columns={'screen_name': 'twitter_handle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remapping party and gender and dealing with independent senators \n",
    "\n",
    "# note: the two independent senators (Bernie Sanders and Angus King) are classified as democratic \n",
    "# for reasons of simplicity and their long-lasting affiliation with the democratic party \n",
    "\n",
    "party_codes = {\"republican\":0, \"democrat\":1, \"independent\":1}\n",
    "senators_df[\"democrat\"] = senators_df[\"party\"].map(party_codes).astype(int)\n",
    "senators_df[\"party\"] = senators_df[\"democrat\"].map({1:\"democrat\", 0:\"republican\"})\n",
    "\n",
    "gender_codes = {\"male\":1, \"female\":0}\n",
    "senators_df[\"male\"] = senators_df[\"gender\"].map(gender_codes).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging selected senator meta info with tweet data\n",
    "\n",
    "merged_df = pd.merge(df, senators_df[[\"state_name\",\"party\", \"democrat\", \"gender\", \"male\", \"ethnicity\", \"religion\", \"openly_lgbtq\", \"date_of_birth\", \"entered_office\", \"twitter_handle\"]], on=\"twitter_handle\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Regional Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading, cleaning and merging regional data \n",
    "# dataset obtained from: https://github.com/cphalpert/census-regions/blob/master/us%20census%20bureau%20regions%20and%20divisions.csv\n",
    "\n",
    "reg_df = pd.read_csv(\"us_census_bureau_regions_and_divisions.csv\")\n",
    "reg_df.rename(columns={\"State\": \"state_name\",\"Region\":\"region\", \"State Code\": \"state_code\"}, inplace=True)\n",
    "reg_df.drop(\"Division\", axis=1, inplace = True)\n",
    "\n",
    "merged_df = pd.merge(merged_df, reg_df, on=\"state_name\", how=\"left\")\n",
    "merged_df.drop(\"id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'text', 'twitter_handle', 'state_name', 'party', 'democrat', 'gender', 'male', 'ethnicity', 'religion', 'openly_lgbtq', 'date_of_birth', 'entered_office', 'state_code', 'region']\n"
     ]
    }
   ],
   "source": [
    "# rearanging columns\n",
    "\n",
    "cols = list(merged_df.columns.values)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['created_at', 'text', 'twitter_handle', 'state_name', 'state_code',  'region', 'party', 'democrat', 'gender', 'male', 'ethnicity', 'religion', 'openly_lgbtq', 'date_of_birth', 'entered_office']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataset\n",
    "\n",
    "merged_df.to_csv(\"tweets_raw.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
